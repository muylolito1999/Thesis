{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lime import lime_tabular\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from PIL import Image as PILImage\n",
    "from io import BytesIO\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dataset con il separatore corretto\n",
    "file_path = '/home/lollo/Thesis/Python/data/data_mapped.csv'\n",
    "data = pd.read_csv(file_path, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "data_encoded = data.copy()\n",
    "\n",
    "# List categorical columns\n",
    "categorical_cols = data_encoded.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Initialize the label encoder\n",
    "label_encoders = {col: LabelEncoder() for col in categorical_cols}\n",
    "\n",
    "# Apply label encoding to each categorical column\n",
    "for col, encoder in label_encoders.items():\n",
    "    data_encoded[col] = encoder.fit_transform(data_encoded[col])\n",
    "\n",
    "# Define the features (X) and the target (y)\n",
    "X = data_encoded.drop('Target', axis=1)\n",
    "y = data_encoded['Target']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_include = [\n",
    "    'Marital status', 'Application order', 'Admission grade',\n",
    "    'Gender', 'Age at enrollment', 'Curricular units 1st sem (evaluations)',\n",
    "    'Curricular units 1st sem (approved)', 'GDP', 'Target'\n",
    "]\n",
    "\n",
    "# Extract the subset of the dataset\n",
    "data_subset = data_encoded[columns_to_include]\n",
    "\n",
    "# Define the features (X) and the target (y) for the subset\n",
    "X_subset = data_subset.drop('Target', axis=1)\n",
    "y_subset = data_subset['Target']\n",
    "\n",
    "# Split the subset dataset into training and test sets\n",
    "X_train_subset, X_test_subset, y_train_subset, y_test_subset = train_test_split(\n",
    "    X_subset, y_subset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree con GridSearchCV per il pruning\n",
    "dt_params = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_grid_search = GridSearchCV(estimator=dt_classifier, param_grid=dt_params, cv=5, scoring='accuracy')\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "best_dt = dt_grid_search.best_estimator_\n",
    "y_pred_dt = best_dt.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"Optimized DecisionTree Accuracy on all Dataset: {accuracy_dt:.2%}\")\n",
    "# Lista delle caratteristiche usate per il training\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Visualizza senza i nomi delle caratteristiche per identificare il problema\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(best_dt, filled=True, \n",
    "          class_names=['Enrolled', 'Dropout', 'Graduated'],  # Usa i nomi dei target corretti\n",
    "          rounded=True, \n",
    "          fontsize=15, \n",
    "          proportion=True,\n",
    "          impurity=False)\n",
    "#plt.show()\n",
    "print(\"Number of features used:\", best_dt.n_features_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri da ottimizzare (stessi di prima)\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Creazione del modello XGBoost\n",
    "xgb_classifier = XGBClassifier(random_state=42, use_label_encoder=False)\n",
    "\n",
    "# Configurazione di RandomizedSearchCV\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_classifier,\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=50,  # Numero di iterazioni, ossia quante combinazioni casuali provare\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=42,  # Per riproducibilit√†\n",
    "    n_jobs=-1  # Usa tutti i processori disponibili per velocizzare la ricerca\n",
    ")\n",
    "\n",
    "# Addestramento del modello con RandomizedSearchCV\n",
    "xgb_random_search.fit(X_train, y_train)\n",
    "\n",
    "# Miglior modello trovato e predizione\n",
    "best_xgb = xgb_random_search.best_estimator_\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "# Calcolo dell'accuratezza\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"Optimized XGBoost Accuracy with RandomizedSearchCV: {accuracy_xgb:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(best_dt, out_file=None, \n",
    "                           feature_names=X.columns.tolist(),  # Usa i nomi delle caratteristiche corrette\n",
    "                           class_names=['Enrolled', 'Dropout', 'Graduated'],  # Usa i nomi delle classi corrette\n",
    "                           filled=True, rounded=True,  \n",
    "                           special_characters=True)  \n",
    "\n",
    "# Converti il dot in un file grafico\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "# Salva l'albero decisionale in un file PDF ad alta risoluzione\n",
    "graph.write_pdf(\"decision_tree.pdf\")\n",
    "\n",
    "# Visualizza l'albero come immagine (opzionale)\n",
    "png_str = graph.create_png()\n",
    "img = PILImage.open(BytesIO(png_str))\n",
    "img.show()\n",
    "# Controlla il numero di caratteristiche usate nel modello\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dell'explainer LIME\n",
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train.values,\n",
    "    feature_names=X.columns.tolist(),\n",
    "    class_names=['Enrolled', 'Dropout', 'Graduated'],\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Genera spiegazioni per le prime 10 istanze del test set\n",
    "for i in range(10):\n",
    "    correct_label_index = y_test.iloc[i]\n",
    "    correct_label = ['Enrolled', 'Dropout', 'Graduated'][correct_label_index]\n",
    "    \n",
    "    # Assicurati che la riga di X_test sia un DataFrame con nomi di colonne\n",
    "    instance = X_test.iloc[[i]]\n",
    "    predicted_label_index = best_dt.predict(instance)[0]\n",
    "    predicted_label = ['Enrolled', 'Dropout', 'Graduated'][predicted_label_index]\n",
    "    \n",
    "    print('Correct:', correct_label)\n",
    "    print('Predicted:', predicted_label)\n",
    "    print(instance.T)  # Mostra le caratteristiche per l'istanza attuale\n",
    "    \n",
    "    explanation = explainer.explain_instance(\n",
    "        data_row=X_test.iloc[i].values,\n",
    "        predict_fn=best_dt.predict_proba,\n",
    "        num_features=10\n",
    "    )\n",
    "\n",
    "    fig = explanation.as_pyplot_figure()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dell'explainer LIME\n",
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train.values,\n",
    "    feature_names=X.columns.tolist(),\n",
    "    class_names=['Enrolled', 'Dropout', 'Graduated'],\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Genera spiegazioni per le prime 5 istanze del test set\n",
    "for i in range(5):\n",
    "    correct_label_index = y_test.iloc[i]\n",
    "    correct_label = ['Enrolled', 'Dropout', 'Graduated'][correct_label_index]\n",
    "    \n",
    "    # Assicurati che la riga di X_test sia un DataFrame con nomi di colonne\n",
    "    instance = X_test.iloc[[i]]\n",
    "    predicted_label_index = best_xgb.predict(instance)[0]\n",
    "    predicted_label = ['Enrolled', 'Dropout', 'Graduated'][predicted_label_index]\n",
    "    \n",
    "    print('Correct:', correct_label)\n",
    "    print('Predicted:', predicted_label)\n",
    "    print(instance.T)  # Mostra le caratteristiche per l'istanza attuale\n",
    "    \n",
    "    explanation = explainer.explain_instance(\n",
    "        data_row=X_test.iloc[i].values,\n",
    "        predict_fn=best_xgb.predict_proba,\n",
    "        num_features=36\n",
    "    )\n",
    "\n",
    "    fig = explanation.as_pyplot_figure()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dell'explainer LIME\n",
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=X_train.values,\n",
    "    feature_names=X.columns.tolist(),\n",
    "    class_names=['Enrolled', 'Dropout', 'Graduated'],\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Inizializzazione di un DataFrame per raccogliere i pesi delle feature\n",
    "lime_weights = pd.DataFrame(columns=['Utente Test'] + X.columns.tolist())\n",
    "\n",
    "# Genera spiegazioni per le prime 5 istanze del test set\n",
    "for i in range(5):\n",
    "    explanation = explainer.explain_instance(\n",
    "        data_row=X_test.iloc[i].values,\n",
    "        predict_fn=best_xgb.predict_proba,\n",
    "        num_features=len(X.columns)  # Considera tutte le feature\n",
    "    )\n",
    "    \n",
    "    # Estrai i pesi delle feature, assicurandoti di gestire correttamente i nomi delle feature\n",
    "    feature_weights = {feature: 0 for feature in X.columns}  # Inizializza tutti i pesi a 0\n",
    "    for feature, weight in explanation.as_list():\n",
    "        # Estrai solo il nome della feature senza il range o altri dettagli aggiuntivi\n",
    "        feature_name = feature.split(' ')[0]\n",
    "        if feature_name in feature_weights:\n",
    "            feature_weights[feature_name] = weight\n",
    "    \n",
    "    # Aggiungi la riga al DataFrame\n",
    "    lime_weights.loc[i] = [f'Utente {i+1}'] + [feature_weights[feature] for feature in X.columns]\n",
    "\n",
    "# Esporta i pesi in un file CSV\n",
    "csv_file_path = '/home/lollo/Thesis/Python/data/lime_weights.csv'\n",
    "lime_weights.to_csv(csv_file_path, index=False)\n",
    "print(f\"Feature weights exported to {csv_file_path}\")\n",
    "\n",
    "# Calcolo della media, deviazione standard e mediana delle feature\n",
    "feature_stats = lime_weights.iloc[:, 1:].agg(['mean', 'std', 'median'])\n",
    "\n",
    "# Calcolo dell'intervallo interquartile (IQR) separatamente\n",
    "iqr = lime_weights.iloc[:, 1:].apply(lambda x: np.percentile(x, 75) - np.percentile(x, 25))\n",
    "\n",
    "# Aggiungi l'IQR alle statistiche delle feature\n",
    "feature_stats.loc['iqr'] = iqr\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Feature Statistics:\")\n",
    "print(feature_stats)\n",
    "\n",
    "# Creazione dei boxplot per la dispersione delle feature\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=lime_weights.iloc[:, 1:])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Boxplot of Feature Weights')\n",
    "plt.show()\n",
    "\n",
    "# Creazione dei boxplot per media e deviazione standard\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=lime_weights.iloc[:, 1:], orient='h')\n",
    "plt.title('Boxplot of Feature Weights by Feature')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot per la mediana e l'intervallo interquartile\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(data=lime_weights.iloc[:, 1:], orient='h', showmeans=True)\n",
    "plt.title('Boxplot with Median and Interquartile Range')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera una spiegazione per un'istanza, ad esempio la prima del test set\n",
    "i = 0  # Indice dell'istanza da spiegare\n",
    "explanation = explainer.explain_instance(\n",
    "    data_row=X_test.iloc[i].values,\n",
    "    predict_fn=best_xgb.predict_proba,\n",
    "    num_features=len(X.columns)  # Considera tutte le feature\n",
    ")\n",
    "\n",
    "# Crea il grafico LIME\n",
    "fig1 = explanation.as_pyplot_figure()\n",
    "\n",
    "# Estrai i pesi delle feature per creare un bar plot\n",
    "feature_weights = {feature: 0 for feature in X.columns}  # Inizializza tutti i pesi a 0\n",
    "for feature, weight in explanation.as_list():\n",
    "    # Estrai solo il nome della feature senza il range o altri dettagli aggiuntivi\n",
    "    feature_name = feature.split(' ')[0]\n",
    "    if feature_name in feature_weights:\n",
    "        feature_weights[feature_name] = weight\n",
    "\n",
    "# Crea un bar plot per i pesi delle feature\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 8))\n",
    "features = list(feature_weights.keys())\n",
    "weights = list(feature_weights.values())\n",
    "\n",
    "sns.barplot(x=weights, y=features, ax=ax2, palette=\"viridis\")\n",
    "ax2.set_title('Feature Weights')\n",
    "ax2.set_xlabel('Weight')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostra entrambi i grafici uno dopo l'altro\n",
    "plt.show(fig1)\n",
    "plt.show(fig2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature weights for 800 users exported to /home/lollo/Thesis/Python/data/lime_weights_800_users.csv\n"
     ]
    }
   ],
   "source": [
    "# Genera spiegazioni LIME per i primi 5 utenti nel set di test\n",
    "num_users = 800  # Cambia questo valore a 800 per elaborare l'intero set di test\n",
    "\n",
    "# Inizializza il DataFrame per salvare i pesi delle feature\n",
    "lime_weights = pd.DataFrame(columns=['Utente Test'] + X.columns.tolist())\n",
    "\n",
    "for i in range(num_users):\n",
    "    explanation = explainer.explain_instance(\n",
    "        data_row=X_test.iloc[i].values,\n",
    "        predict_fn=best_xgb.predict_proba,\n",
    "        num_features=len(X.columns)  # Considera tutte le feature\n",
    "    )\n",
    "    \n",
    "    # Estrai i pesi delle feature\n",
    "    feature_weights = {feature: 0 for feature in X.columns}  # Inizializza tutti i pesi a 0\n",
    "    for feature, weight in explanation.as_list():\n",
    "        # Estrai solo il nome della feature senza il range o altri dettagli aggiuntivi\n",
    "        feature_name = feature.split(' ')[0]\n",
    "        if feature_name in feature_weights:\n",
    "            feature_weights[feature_name] = weight\n",
    "    \n",
    "    # Aggiungi la riga al DataFrame\n",
    "    lime_weights.loc[i] = [f'Utente {i+1}'] + [feature_weights[feature] for feature in X.columns]\n",
    "\n",
    "# Esporta i pesi in un file CSV\n",
    "csv_file_path = '/home/lollo/Thesis/Python/data/lime_weights_800_users.csv'\n",
    "lime_weights.to_csv(csv_file_path, index=False)\n",
    "print(f\"Feature weights for 800 users exported to {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
